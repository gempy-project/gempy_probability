{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\nfrom pyvista import set_plot_theme\nset_plot_theme('document')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Overview to Bayesian Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model definition\nGenerally, models represent an abstraction of reality to answer a specific question, to fulfill a certain purpose, or to simulate (mimic) a proces or multiple processes. What models share is the aspiration to be as realistic as possible, so they can be used for prognoses and to better understand a real-world system.\n\nFitting of these models to acquired measurements or observations is called calibration and a standard procedure for improving a models reliability (**to answer the question it was designed for**).\n\nModels can also be seen as a general descriptor of correlation of observations in multiple dimensions. Complex systems with generally sparse data coverage (e.g. the subsurface) are difficult to reliably encode from the real-world in the numerical abstraction, i.e. a computational model.\n\nIn a probabilistic framework, a model is a framework of different input distributions, which, as an output, has another probability distribution.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import io\n\nimport matplotlib.pyplot as plt\nimport pyro\nimport pyro.distributions as dist\nimport torch\nfrom PIL import Image\n\n# sphinx_gallery_thumbnail_number = -1\n\npyro.set_rng_seed(4003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simplest probabilistic modeling\n\nConsider the simplest probabilistic model where the output $y$ of a model is a distribution.\nLet's assume, $y$ is a normal distribution, described by a mean $\\mu$ and a standard deviation $\\sigma$.\nUsually, those are considered scalar values, but they themselves can be distributions.\nThis will yield a change of the width and position of the normal distribution $y$ with each iteration.\n\nAs a reminder, a normal distribution is defined as:\n\n\\begin{align}y = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\, e^{-\\frac{(x - \\mu)^2}{2 \\sigma ^2}}\\end{align}\n\n* $\\mu$: mean (Normal distribution)\n* $\\sigma$: standard deviation (Gamma distribution, Gamma log-likelihood)\n* $y$: Normal distribution\n\nWith this constructed model, we are able to infer which model parameters will fit observations better by optimizing for regions with high density mass.\nIn addition (or even substituting) to data observations, informative values like prior simulations or expert knowledge can pour into the construction of the first $y$ distribution, the `prior`.\n\nThere isn't a limitation about how \"informative\" a prior can or must be. Depending on the variance of the model's parameters\nand on the number of observations, a model will be more `prior driven` or `data driven`.\n\nLet's set up a `Pyro` model using the `thickness_observation` from above as observations and with $\\mu$ and $\\sigma$ being:\n\n* $\\mu = \\text{Normal distribution with mean } 2.08 \\text{ and standard deviation } 0.07$\n* $\\sigma = \\text{Gamma distribution with } \\alpha = 0.3 \\text{ and } \\beta = 3$\n* $y = \\text{Normal distribution with } \\mu, \\sigma \\text{ and thickness_observation_list as observations}$\n\nA [Gamma distribution](https://docs.pymc.io/en/latest/api/distributions/generated/pymc.Gamma.html)_\ncan also be expressed by mean and standard deviation with $\\alpha = \\frac{\\mu^2}{\\sigma^2}$ and\n$\\beta = \\frac{\\mu}{\\sigma^2}$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def model(distributions_family, data):\n    if distributions_family == \"normal_distribution\":\n        param_mean = pyro.param('$\\\\mu_{prior}$', torch.tensor(2.07))\n        param_std = pyro.param('$\\\\sigma_{prior}$', torch.tensor(0.07), constraint=dist.constraints.positive)\n        mu = pyro.sample('$\\\\mu_{likelihood}$', dist.Normal(param_mean, param_std))\n    elif distributions_family in \"uniform_distribution\":\n        param_low = pyro.param('$\\\\mu_{prior}$', torch.tensor(0))\n        param_high = pyro.param('$\\\\sigma_{prior}$', torch.tensor(10))\n        mu = pyro.sample('$\\\\mu_{likelihood}$', dist.Uniform(param_low, param_high))\n    else:\n        raise ValueError(\"distributions_family must be either 'normal_distribution' or 'uniform_distribution'\")\n    param_concentration = pyro.param('$\\\\alpha_{prior}$', torch.tensor(0.3), constraint=dist.constraints.positive)\n    param_rate = pyro.param('$\\\\beta_{prior}$', torch.tensor(3), constraint=dist.constraints.positive)\n    \n    sigma = pyro.sample('$\\\\sigma_{likelihood}$', dist.Gamma(param_concentration, param_rate))\n    y = pyro.sample('$y$', dist.Normal(mu, sigma), obs=data)\n    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_obs = torch.tensor([2.12])\ny_obs_list = torch.tensor([2.12, 2.06, 2.08, 2.05, 2.08, 2.09,\n                           2.19, 2.07, 2.16, 2.11, 2.13, 1.92])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Render the model as a graph\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = pyro.render_model(\n    model=model,\n    model_args=(\"normal_distribution\", y_obs_list,),\n    render_params= True,\n    render_distributions=True\n)\ngraph.attr(dpi='300')\n# Convert the graph to a PNG image format\ns = graph.pipe(format='png')\n\n# Open the image with PIL\nimage = Image.open(io.BytesIO(s))\n\n# Plot the image with matplotlib\nplt.figure(figsize=(10, 4))\nplt.imshow(image)\nplt.axis('off')  # Turn off axis\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next:\n- Several Observations: (:doc:`1.2_Intro_to_Bayesian_Inference`)\n- One Observation: (:doc:`1.1_Intro_to_Bayesian_Inference`)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# License\nThe code in this case study is copyrighted by Miguel de la Varga and licensed under the new BSD (3-clause) license:\n\nhttps://opensource.org/licenses/BSD-3-Clause\n\nThe text and figures in this case study are copyrighted by Miguel de la Varga and licensed under the CC BY-NC 4.0 license:\n\nhttps://creativecommons.org/licenses/by-nc/4.0/\nMake sure to replace the links with actual hyperlinks if you're using a platform that supports it (e.g., Markdown or HTML). Otherwise, the plain URLs work fine for plain text.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}