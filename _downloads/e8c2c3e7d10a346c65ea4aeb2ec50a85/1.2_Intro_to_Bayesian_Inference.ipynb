{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\nfrom pyvista import set_plot_theme\nset_plot_theme('document')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Normal Prior, several observations\n\n# We previously mentioned that we can optimize this model to better fit observations.\n For this, we generate a prior from the model we previously created, the one with \n multiple observations from `y_obs_list`.\n# In the following, samples are iteratively drawn and evaluated to form the posterior. \nThis generates what is called a \"trace\". A predictive Posterior distribution is then \ngenerated from said trace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import arviz as az\nimport matplotlib.pyplot as plt\nimport pyro\nimport torch\nfrom matplotlib.ticker import StrMethodFormatter\n\nfrom gempy_probability.plot_posterior import PlotPosterior\nfrom _aux_func import infer_model\naz.style.use(\"arviz-doc\")\n\n# Sample observations\ny_obs_list = torch.tensor([2.12, 2.06, 2.08, 2.05, 2.08, 2.09,\n                           2.19, 2.07, 2.16, 2.11, 2.13, 1.92])\npyro.set_rng_seed(4003)\n\n# Infer model from the observations\naz_data = infer_model(\n    distributions_family=\"normal_distribution\",\n    data=y_obs_list\n)\n\n# Plot the trace of the inference process\naz.plot_trace(az_data)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raw observations\nThe behavior of this chain is controlled by the observations we fed into the model. \nLet's have a look at the observations and how they are distributed:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = PlotPosterior(az_data)\np.create_figure(figsize=(9, 3), joyplot=False, marginal=False)\np.plot_normal_likelihood(\n    mean='$\\\\mu_{likelihood}$',\n    std='$\\\\sigma_{likelihood}$',\n    obs='$y$',\n    iteration=-1,\n    hide_bell=True\n)\np.likelihood_axes.set_xlim(1.90, 2.2)\np.likelihood_axes.xaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))\nfor tick in p.likelihood_axes.get_xticklabels():\n    tick.set_rotation(45)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bulk of observations is between 2.05 and 2.15, with one outlier at 1.92.\n\n## Final inference\nLet's now plot the inferred posterior distribution (i.e., the last sample iteration) \nand the observations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = PlotPosterior(az_data)\np.create_figure(figsize=(9, 3), joyplot=False, marginal=False)\np.plot_normal_likelihood(\n    mean='$\\\\mu_{likelihood}$',\n    std='$\\\\sigma_{likelihood}$',\n    obs='$y$',\n    iteration=-1,\n    hide_bell=False\n)\np.likelihood_axes.set_xlim(1.70, 2.40)\np.likelihood_axes.xaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))\nfor tick in p.likelihood_axes.get_xticklabels():\n    tick.set_rotation(45)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bell-peak is centered on a cluster of observations, but the outlier at 1.92 \nshifts the distribution slightly.\nJoyplot\n-------\nTo visualize the change in distribution across iterations, we use a joyplot. This allows \nus to see how the mean ($\\\\mu$) and standard deviation ($\\\\sigma$) change over time with \nprogressive sampling:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = PlotPosterior(az_data)\np.create_figure(figsize=(9, 9), joyplot=True, marginal=False, likelihood=False, n_samples=31)\np.plot_joy(\n    var_names=('$\\\\mu_{likelihood}$', '$\\\\sigma_{likelihood}$'),\n    obs='$y$',\n    iteration=14\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following animation shows how the distribution evolves during sampling. Darker colors \nrepresent an increase in likelihood as the Markov Chain explores the probability space.\n\n<img src=\"file://_static/joyplot_2.gif\">\n\n## Joint Probability\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = PlotPosterior(az_data)\np.create_figure(figsize=(9, 5), joyplot=False, marginal=True, likelihood=True)\np.plot_marginal(\n    var_names=['$\\\\mu_{likelihood}$', '$\\\\sigma_{likelihood}$'],\n    plot_trace=False,\n    credible_interval=0.95,\n    kind='kde',\n    joint_kwargs={'contour': True, 'pcolormesh_kwargs': {}},\n    joint_kwargs_prior={'contour': False, 'pcolormesh_kwargs': {}}\n)\np.plot_normal_likelihood(\n    mean='$\\\\mu_{likelihood}$',\n    std='$\\\\sigma_{likelihood}$',\n    obs='$y$',\n    iteration=-1,\n    hide_lines=True\n)\np.likelihood_axes.set_xlim(1.70, 2.40)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling Process\nBelow is a gif of the first 100 samples, starting from the 10th iteration:\n\n\n<img src=\"file://_static/sampling_2.gif\">\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p3 = PlotPosterior(az_data)\np3.create_figure(figsize=(15, 13), joyplot=True, marginal=True, likelihood=True, n_samples=19)\np3.plot_posterior(\n    prior_var=['$\\\\mu_{likelihood}$', '$\\\\sigma_{likelihood}$'], \n    like_var=['$\\\\mu_{likelihood}$', '$\\\\sigma_{likelihood}$'],\n    obs='$y$',\n    iteration=-5,\n    marginal_kwargs={\n            'plot_trace'       : True,\n            'credible_interval': .95,\n            'kind'             : 'kde',\n            \"joint_kwargs\"     : {\n                    'contour'          : True,\n                    'pcolormesh_kwargs': {}\n            },\n    }\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# License\nThe code in this case study is copyrighted by Miguel de la Varga and licensed under the \nnew BSD (3-clause) license:\n\nhttps://opensource.org/licenses/BSD-3-Clause\n\nThe text and figures in this case study are copyrighted by Miguel de la Varga and licensed \nunder the CC BY-NC 4.0 license:\n\nhttps://creativecommons.org/licenses/by-nc/4.0/\nMake sure to replace the links with actual hyperlinks if you're using a platform that supports \nit (e.g., Markdown or HTML). Otherwise, the plain URLs work fine for plain text.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}