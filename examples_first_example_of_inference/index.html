<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>﻿1 - First example of inference &#8212; GemPy 2024.2.0.3.dev0+gf344a731.d20240626 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=270d38c7" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=bd57e1ea"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Overview to Bayesian Inference" href="1-bayesian_basics/1.0_Overview_to_Bayesian_Inference.html" />
    <link rel="prev" title="Intro" href="../examples_intro/index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="first-example-of-inference">
<h1>﻿1 - First example of inference<a class="headerlink" href="#first-example-of-inference" title="Link to this heading">¶</a></h1>
<div class="sphx-glr-thumbnails"></div><section id="simple-example-using-pyro-moving-the-bell">
<h2>Simple example using Pyro: Moving the bell<a class="headerlink" href="#simple-example-using-pyro-moving-the-bell" title="Link to this heading">¶</a></h2>
<p>We can sum up the workflow above in the following steps:</p>
<ol class="arabic simple">
<li><p>We chose a probabilistic density function family <span class="math notranslate nohighlight">\(\pi\)</span>—e.g., of the Gaussian or uniform families, <span class="math notranslate nohighlight">\(\pi_S(\theta)\)</span> to approximate the <em>data generating process</em>.</p></li>
<li><p>We narrow the possible values of <span class="math notranslate nohighlight">\(\theta\)</span>—mean and standard deviation in the case of a Normal distribution—with domain knowledge (notice that although <span class="math notranslate nohighlight">\(\pi_S\)</span> is a subset of <span class="math notranslate nohighlight">\(\pi_P\)</span>, its size is infinite and therefore exhaustive approaches are unfeasible).</p></li>
<li><p>We use Bayesian inference to condition the prior estimations to our samples of reality <span class="math notranslate nohighlight">\(\pi_S(\tilde{y})|\theta)\)</span>.</p></li>
</ol>
<p>Back to the thickness example, we need to assign some prior distribution to the mean and standard deviation of the thickness; for example, we can take a naive approach and use the values of the crust, or on the contrary, we could use only the data we gathered and maybe one or two references. At this stage, there is not one single valid answer. To analyze how different priors affect, we will use 4 possible configurations, 2 of them using a normal distribution for the mean of a Gaussian likelihood function and the other 2 using a uniform distribution—between 0 and 10 in order to keep it as uninformative as possible while still giving valid physical values:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Normal</strong></p></th>
<th class="head"><p><strong>Uniform</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\theta_\mu \sim \mathcal{N}(\mu=2.08, \sigma=0.07)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\theta_\mu \sim \mathcal{U}(a=0, b=10)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\theta_\sigma \sim \Gamma(\alpha=0.3, \beta=3)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\theta_\sigma \sim \Gamma(=1.0, \beta=0.7)\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\pi_S(y|\theta) \sim \mathcal{N}(\mu=\theta_\mu, \sigma=\theta_\sigma)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\pi_S(y|\theta) \sim \mathcal{N}(\mu=\theta_\mu, \sigma=\theta_\sigma)\)</span></p></td>
</tr>
</tbody>
</table>
<p>The standard deviation is in both cases a quite uninformative Gamma distribution. Besides using different probability functions to describe one of the model parameters, we have also repeated the same simulation either using one observation, <span class="math notranslate nohighlight">\(\tilde{y}\)</span> at 2.12 m, or 10 observation spread randomly (as random as a human possibly can) around 2.10. Figure <code class="xref std std-numref docutils literal notranslate"><span class="pre">fig-models1</span></code> shows the joint prior (in blue) and posterior (in red) distributions for the 4 probabilistic models, as well as, the maximum likelihood function.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Overview to Bayesian Inference"><img alt="" src="../_images/sphx_glr_1.0_Overview_to_Bayesian_Inference_thumb.png" />
<p><a class="reference internal" href="1-bayesian_basics/1.0_Overview_to_Bayesian_Inference.html#sphx-glr-examples-first-example-of-inference-1-bayesian-basics-1-0-overview-to-bayesian-inference-py"><span class="std std-ref">Overview to Bayesian Inference</span></a></p>
  <div class="sphx-glr-thumbnail-title">Overview to Bayesian Inference</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Normal Prior, single observation"><img alt="" src="../_images/sphx_glr_1.1_Intro_to_Bayesian_Inference_thumb.png" />
<p><a class="reference internal" href="1-bayesian_basics/1.1_Intro_to_Bayesian_Inference.html#sphx-glr-examples-first-example-of-inference-1-bayesian-basics-1-1-intro-to-bayesian-inference-py"><span class="std std-ref">Normal Prior, single observation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Normal Prior, single observation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="# We previously mentioned that we can optimize this model to better fit observations.  For this..."><img alt="" src="../_images/sphx_glr_1.2_Intro_to_Bayesian_Inference_thumb.png" />
<p><a class="reference internal" href="1-bayesian_basics/1.2_Intro_to_Bayesian_Inference.html#sphx-glr-examples-first-example-of-inference-1-bayesian-basics-1-2-intro-to-bayesian-inference-py"><span class="std std-ref">Normal Prior, several observations</span></a></p>
  <div class="sphx-glr-thumbnail-title">Normal Prior, several observations</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Uniform Prior, single observation"><img alt="" src="../_images/sphx_glr_1.3_Intro_to_Bayesian_Inference_thumb.png" />
<p><a class="reference internal" href="1-bayesian_basics/1.3_Intro_to_Bayesian_Inference.html#sphx-glr-examples-first-example-of-inference-1-bayesian-basics-1-3-intro-to-bayesian-inference-py"><span class="std std-ref">Uniform Prior, single observation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Uniform Prior, single observation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Uniform Prior, several observations"><img alt="" src="../_images/sphx_glr_1.4_Intro_to_Bayesian_Inference_thumb.png" />
<p><a class="reference internal" href="1-bayesian_basics/1.4_Intro_to_Bayesian_Inference.html#sphx-glr-examples-first-example-of-inference-1-bayesian-basics-1-4-intro-to-bayesian-inference-py"><span class="std std-ref">Uniform Prior, several observations</span></a></p>
  <div class="sphx-glr-thumbnail-title">Uniform Prior, several observations</div>
</div></div><div class="toctree-wrapper compound">
</div>
<section id="result-description-and-some-words-about-uninformative-prior">
<h3>﻿Result description and some words about uninformative prior<a class="headerlink" href="#result-description-and-some-words-about-uninformative-prior" title="Link to this heading">¶</a></h3>
<p>The first thing that comes to mind by looking at the results is that the model choice has a clear impact on the final result although the more observations are included the more similar both models get indistinctively of the priors. Therefore, it is interesting to compare the two cases with just one observation (Figure <code class="xref std std-numref docutils literal notranslate"><span class="pre">fig-models1</span></code>, above), curiously enough, the probabilistic model with the normal distribution underrepresents the model uncertainty—assuming that the simulations c) and d) are closer to the real data generating process—while the uniform clearly portrays too much variance. Once again, considering that the only window to the data generating model are indirect observation of a phenomenon, in regimes of low and/or noisy data points there is only so much information we can extract from the data. This is to say that there are not truly uninformative prior distributions—although there are theoretical limits such as maximum entropy—and therefore the best we can do is to use the knowledge of the practitioner as <em>first educated guess</em>.</p>
</section>
</section>
<section id="type-of-probabilistic-models">
<h2>Type of probabilistic models<a class="headerlink" href="#type-of-probabilistic-models" title="Link to this heading">¶</a></h2>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/Model_type.png"><img alt="../_images/Model_type.png" src="../_images/Model_type.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Example of computational graph expressing simple probabilistic model
:name: fig-model-type</span><a class="headerlink" href="#id3" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Depending on the type of phenomenon analyzed and the model complexity, two different categories of problems emerge: Wide data and Long data. Long data problems are characterized by a large number of repetitions of the same phenomenon, allowing all sorts of machine learning and big data approaches. Wide data, on the other hand, features few repetitions of several phenomena that rely on complex mathematical models to relate them <a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. Structural geology is an unequivocal example of the second case due to the sheer size and scale it is aimed to be modeled. For this reason, a systematic way to include domain knowledge into the system via informative prior distributions becomes a powerful tool in the endeavor of integrating as much information as possible in a single mathematical model.</p>
</section>
<section id="priors-as-joint-probability">
<h2>Priors as joint probability<a class="headerlink" href="#priors-as-joint-probability" title="Link to this heading">¶</a></h2>
<p>In low data regimes where domain knowledge plays such an essential role, it is crucial to include as much <strong>coherent</strong> information as possible. Simply giving the best estimate of the data generating function may work for simple systems where our brains are capable to relate available data and knowledge. Nevertheless, as complexity rises, a more systematic way of combining data and knowledge becomes fundamental. Models are in essence a tool to generate best guesses following mathematical axioms. This view of models as tools to combine different sources of data and knowledge to help to define the best guess of a latent data generating process may seem an unnecessary convoluted way to explain Bayesian statistics. However, in our opinion, this view helps to change the perspective of prior distributions—from terms of “belief” as a source of bias—to a more general perspective of using joint probability as a means to combine complex mathematical models and observations in a mathematically sound and transparent manner.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>The term comes from the shape of the database. If you imagine columns to be different phenomena—i.e., properties, including space and time—and rows as repetition of measurements, it will be wide data when the proportion of columns vs. rows is large, and it will be long data in the opposite case.</p>
</aside>
</aside>
<section id="license">
<h3>License<a class="headerlink" href="#license" title="Link to this heading">¶</a></h3>
<p>The code in this case study is copyrighted by Miguel de la Varga and licensed under the new BSD (3-clause) license:</p>
<p><a class="reference external" href="https://opensource.org/licenses/BSD-3-Clause">https://opensource.org/licenses/BSD-3-Clause</a></p>
<p>The text and figures in this case study are copyrighted by Miguel de la Varga and licensed under the CC BY-NC 4.0 license:</p>
<p><a class="reference external" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a>
Make sure to replace the links with actual hyperlinks if you’re using a platform that supports it (e.g., Markdown or HTML). Otherwise, the plain URLs work fine for plain text.</p>
<div class="sphx-glr-thumbnails"></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/logos/gempy.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=gempy-project&repo=gempy&type=star&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Probabilistic modeling for Structural Geology</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../examples_intro/index.html">Intro</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">﻿1 - First example of inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#simple-example-using-pyro-moving-the-bell">Simple example using Pyro: Moving the bell</a></li>
<li class="toctree-l2"><a class="reference internal" href="#type-of-probabilistic-models">Type of probabilistic models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#priors-as-joint-probability">Priors as joint probability</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples_basic_geology/index.html">﻿2 - Probabilistic Geomodel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_probabilistic_inversion/index.html">﻿3 - Probabilistic Geophysics (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_utils/index.html">﻿Utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html">Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html#data-classes">Data Classes</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../examples_intro/index.html" title="previous chapter">Intro</a></li>
      <li>Next: <a href="1-bayesian_basics/1.0_Overview_to_Bayesian_Inference.html" title="next chapter">Overview to Bayesian Inference</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023-2024, Gempy Probability Developers.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.2.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../_sources/examples_first_example_of_inference/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>