<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Intro &#8212; GemPy 2023.2.0b1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=270d38c7" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=e5fbc548" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=39c4bfd0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="﻿1 - First example of inference" href="../examples_first_example_of_inference/index.html" />
    <link rel="prev" title="Installation" href="../installation.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="intro">
<h1>Intro<a class="headerlink" href="#intro" title="Permalink to this heading">¶</a></h1>
<p>It is not possible to understand the scope and limitations of probability machine learning without a precise definition of what we refer by model—at least in the context of this paper. Probabilistic models allow to integrate mathematical abstractions and observations of the physical world on a rigorous and consistent manner. A model, <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>, simply put, is a symbolic representation of the world directed to find causality in order to enable predictions, in dimensions—e.g locations or time—where we do not have direct access to physical observations.</p>
<p>Due to the limitations gathering independent information from reality, mapping mathematical models—which is abstract logic—to quantifiable observations of reality is not trivial. Probabilistic modeling aims to reconcile this duality by assuming that any observation <span class="math notranslate nohighlight">\(y\)</span> has been generated by a latent random process. This is nothing else than trying to quantify our ignorance of a given system. In other words, since neither a perfect mathematical descriptor of reality nor an isolated observation of a specific phenomenon are attainable, any attempt to describe complex systems deterministically are futile. This does not mean that modelling is a hopeless effort. Reality is not random, and models and observations contain correlated information that we can relate mathematically. However, information correlation is—except in cases of perfect information—a probabilistic phenomenon and we should treat it as such. Now, we will introduce some basic probabilistic models hoping to clarify some of the concepts and to define the nomenclature used in this paper [^1_].</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/computational_graph_example.png"><img alt="../_images/computational_graph_example.png" src="../_images/computational_graph_example.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Example of computational graph expressing simple probabilistic model</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="problem-introduction">
<h2>Problem introduction<a class="headerlink" href="#problem-introduction" title="Permalink to this heading">¶</a></h2>
<section id="back-to-geology-how-thick-is-my-layer">
<h3>Back to geology: How thick is my layer?<a class="headerlink" href="#back-to-geology-how-thick-is-my-layer" title="Permalink to this heading">¶</a></h3>
<p>Let’s start with the simplest model in structural geology we have been able to come up and trying to be agonizingly pedantic about it. You want to model the thickness of one layer on one specific outcrop and we want to be right. To be sure, we will go during 10 years once per month with a tape measure and we will write down the value on the tape. Finally, 10 years have passed and we are ready to give a conclusive answer. However, out of 120 observations, there are 120 distinct numbers. The first thing we need to reconcile is that any measurements of any phenomenon are isolated. Changes in the environment—maybe some earthquake—or in the way we gather the data—we changed the measurement device or simply our eyes got tired over the years—will also influence which observations <span class="math notranslate nohighlight">\(y\)</span> we end up with. No matter the exact source of the variability, all these processes define the Observation space, <span class="math notranslate nohighlight">\(Y\)</span>. Any observational space is going to have some kind of structure that can be modeled for a probability density function called <em>data generating process</em> <span class="math notranslate nohighlight">\(\pi^\dagger\)</span>. In other words, there is a latent process—which is a complex combination of several dynamics and limitations—that every time we perform a measurement will yield a value following a certain probability function. Now, to the question what is the thickness of the layer, the answer that better describe the 120 measurements will have to be a probability density function instead of a single value but how can we know what probability function is the right one?</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../_images/Model_space1.png"><img alt="../_images/Model_space1.png" src="../_images/Model_space1.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Example of a probability density function fitting to observational data.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>The probabilistic model</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Add somewhere that <span class="math notranslate nohighlight">\(\theta\)</span> in probabilistic modeling is a random variable and therefore a simplification of saying mean and standard deviation.</p>
</div>
<p>The <em>data generating process</em> is latent. Therefore, it will be impossible to perfectly describe it. At this point, we need to compromise and start to make assumptions and choose a model. Any probabilistic family, <span class="math notranslate nohighlight">\(\pi(y;\Theta)\)</span>, consist of two distinct sets: observations, <span class="math notranslate nohighlight">\(y\)</span>, and model parameters <span class="math notranslate nohighlight">\(\Theta\)</span>. Depending on which set is fixed, we obtain either (i) the forward view: for a given set of <span class="math notranslate nohighlight">\(\Theta\)</span> there is a probability of sampling <span class="math notranslate nohighlight">\(y\)</span>, or (ii) the inverse view: the observed <span class="math notranslate nohighlight">\(y\)</span> is so much likely for these values of <span class="math notranslate nohighlight">\(\Theta\)</span>. Here, we assume that we perform inverse statistics and hence we fix the observations <span class="math notranslate nohighlight">\(y\)</span>. For this example, we can assume that the generating process belongs to the Gaussian family:</p>
<div class="math notranslate nohighlight">
\[\pi_S(y; \mu, \sigma) = \frac{1}{{\sigma \sqrt {2\pi } }}e^{{{ - \left( {y - \mu } \right)^2 } / {2\sigma ^2 }}}\]</div>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/Model_space2.png"><img alt="../_images/Model_space2.png" src="../_images/Model_space2.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">The joy plot has to be random!</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>No matter which probability density function we choose, for real applications, we will never find the exact data generating process—neither will we be able to say if we have found it for that matter—due to an oversimplification of reality. In Chapter <cite>sec:model_selection</cite> we will delve into this topic.</p>
<p>Once the model is defined we need to infer the set of parameters <span class="math notranslate nohighlight">\(\Theta\)</span> of the family of density functions over the observational space, <span class="math notranslate nohighlight">\(\pi_S(y;\Theta)\)</span>. In the case of the normal family, we need to infer the value of the mean, <span class="math notranslate nohighlight">\(\mu\)</span>, and standard deviation, <span class="math notranslate nohighlight">\(\sigma\)</span>. Up to this point, all the description of the probabilistic modelling is agnostic in relation to Frequentist or Bayesian views.</p>
<p><strong>Bayesian inference as formalisation of the above</strong></p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/Model_space3.png"><img alt="../_images/Model_space3.png" src="../_images/Model_space3.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Bayesian inference applied to the problem.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Bayesian inference is based on using the actual observations of reality, <span class="math notranslate nohighlight">\(\tilde{y}\)</span>, as conditional probability of a prior definition of <span class="math notranslate nohighlight">\(\Theta\)</span>. This construct enables to infer—learn—which model parameters will fit better the observation by “optimising’’ for regions of high-density mass. The simplicity of Bayes equation hides an elegant modular formulation that allows infinite complex trees of conditional probability. However, the dependency of a multidimensional integral has limited its adoption in engineering and other highly complex models. Thankfully, due to the latest advancements in algorithms and computing resources, we are at the dawn of scaling Bayesian networks to a level capable to substantially</p>
</section>
</section>
<section id="license">
<h2>License<a class="headerlink" href="#license" title="Permalink to this heading">¶</a></h2>
<p>The code in this case study is copyrighted by Miguel de la Varga and licensed under the new BSD (3-clause) license:</p>
<p><a class="reference external" href="https://opensource.org/licenses/BSD-3-Clause">https://opensource.org/licenses/BSD-3-Clause</a></p>
<p>The text and figures in this case study are copyrighted by Miguel de la Varga and licensed under the CC BY-NC 4.0 license:</p>
<p><a class="reference external" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a>
Make sure to replace the links with actual hyperlinks if you’re using a platform that supports it (e.g., Markdown or HTML). Otherwise, the plain URLs work fine for plain text.</p>
<div class="sphx-glr-thumbnails"></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/logos/gempy.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=gempy-project&repo=gempy&type=star&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Probabilistic modeling for Structural Geology</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Intro</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#problem-introduction">Problem introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#license">License</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples_first_example_of_inference/index.html">﻿1 - First example of inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_basic_geology/index.html">﻿2 - Probabilistic Geomodel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_probabilistic_inversion/index.html">﻿3 - Probabilistic Geophysics (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_utils/index.html">﻿Utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html">Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html#data-classes">Data Classes</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../installation.html" title="previous chapter">Installation</a></li>
      <li>Next: <a href="../examples_first_example_of_inference/index.html" title="next chapter">﻿1 - First example of inference</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023-2023, Gempy Probability Developers.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../_sources/examples_intro/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>