
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples_intro/1-bayesian_basics/1_Intro_to_Bayesian_Inference.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_intro_1-bayesian_basics_1_Intro_to_Bayesian_Inference.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_intro_1-bayesian_basics_1_Intro_to_Bayesian_Inference.py:


Intro to Bayesian Inference
===========================

.. GENERATED FROM PYTHON SOURCE LINES 7-24

.. code-block:: python3


    import torch
    import pyro
    import pyro.distributions as dist
    from matplotlib.ticker import StrMethodFormatter
    from pyro.infer import MCMC, NUTS, Predictive
    import arviz as az
    import matplotlib.pyplot as plt

    from gempy_probability.plot_posterior import PlotPosterior

    y_obs = torch.tensor([2.12])
    y_obs_list = torch.tensor([2.12, 2.06, 2.08, 2.05, 2.08, 2.09,
                               2.19, 2.07, 2.16, 2.11, 2.13, 1.92])
    pyro.set_rng_seed(4003)









.. GENERATED FROM PYTHON SOURCE LINES 25-56

.. code-block:: python3

    def model(conf, data):
        if conf in ['n_s', 'n_o']:
            mu = pyro.sample('$\mu$', dist.Normal(2.08, 0.07))
        elif conf in ['u_s', 'u_o']:
            mu = pyro.sample('$\mu$', dist.Uniform(0, 10))
        sigma = pyro.sample('$\sigma$', dist.Gamma(0.3, 3))
        y = pyro.sample('$y$', dist.Normal(mu, sigma), obs=data)
        return y


    def infer_model(config, data):
        # 1. Prior Sampling
        prior = Predictive(model, num_samples=100)(config, data)
        # 2. MCMC Sampling
        nuts_kernel = NUTS(model)
        mcmc = MCMC(nuts_kernel, num_samples=100, warmup_steps=100)  # Assuming 1000 warmup steps
        mcmc.run(config, data)
        # Get posterior samples
        posterior_samples = mcmc.get_samples()
        # 3. Sample from Posterior Predictive
        posterior_predictive = Predictive(model, posterior_samples)(config, data)
        # %%
        az_data = az.from_pyro(
            posterior=mcmc,
            prior=prior,
            posterior_predictive=posterior_predictive
        )

        return az_data









.. GENERATED FROM PYTHON SOURCE LINES 57-70

The *data generating process* is latent. Therefore, it will be impossible to perfectly describe it. At this point,
we need to compromise and start to make assumptions and choose a model.
Any probabilistic family, \( \pi(y;\varTheta) \), consists of two distinct sets: observations, \( y \) and model
parameters \( \varTheta \). Depending on which set is fixed, we obtain either (i) the forward view: for a given set 
of \( \varTheta \) there is a probability of sample \( y \) or (ii) the inverse view: the observed \( y \) is so much 
likely for these values of \( \varTheta \). Discussing the nuances between assuming that the model emerge from 
observations or if the observation are generated by the model are beyond the scope of this paper. Here, we stand on 
an eclectic position, arguing that these options are not mutually excluding, any (prior) model that the brain can 
come up are substantiated—in one way or another—on direct observations of the present or in observations of the past 
consolidated in form of experience. This reconciliation of statistical approaches it is formalised as Empirical Bayes. 
In chapter XX, we will deepen on this topic while describing the proposed network for structural geology, for now, 
let’s assume that we perform inverse statistics and hence we fix the observations \( y \). For this example we can 
assume that the generating process belongs to the Gaussian family:

.. GENERATED FROM PYTHON SOURCE LINES 72-73

TODO: Make the graph of the model

.. GENERATED FROM PYTHON SOURCE LINES 73-78

.. code-block:: python3


    az_data = infer_model("u_s", y_obs_list)
    az.plot_trace(az_data)
    plt.show()




.. image-sg:: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_001.png
   :alt: $\mu$, $\mu$, $\sigma$, $\sigma$
   :srcset: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Warmup:   0%|          | 0/200 [00:00, ?it/s]    Warmup:  10%|#         | 20/200 [00:00, 167.92it/s, step size=1.11e-02, acc. prob=0.709]    Warmup:  18%|#8        | 37/200 [00:00, 165.09it/s, step size=3.82e-02, acc. prob=0.756]    Warmup:  27%|##7       | 54/200 [00:00, 125.77it/s, step size=1.87e-02, acc. prob=0.761]    Warmup:  34%|###4      | 68/200 [00:00, 127.94it/s, step size=1.52e-02, acc. prob=0.765]    Warmup:  41%|####1     | 82/200 [00:00, 125.00it/s, step size=2.26e-02, acc. prob=0.771]    Warmup:  48%|####7     | 95/200 [00:00, 125.01it/s, step size=5.42e-01, acc. prob=0.762]    Sample:  64%|######3   | 127/200 [00:00, 182.00it/s, step size=5.24e-01, acc. prob=0.933]    Sample:  81%|########1 | 162/200 [00:00, 230.75it/s, step size=5.24e-01, acc. prob=0.943]    Sample:  98%|#########8| 197/200 [00:01, 265.34it/s, step size=5.24e-01, acc. prob=0.944]    Sample: 100%|##########| 200/200 [00:01, 193.12it/s, step size=5.24e-01, acc. prob=0.944]

    /home/leguark/.virtualenvs/gempy-geotop-pilot/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: gempy.-version- is an invalid version and will not be supported in a future release
      warnings.warn(
    /home/leguark/.virtualenvs/gempy-geotop-pilot/lib/python3.10/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 79-88

.. code-block:: python3

    p = PlotPosterior(az_data)
    p.create_figure(figsize=(9, 3), joyplot=False, marginal=False)
    p.plot_normal_likelihood('$\mu$', '$\sigma$', '$y$', iteration=-1, hide_bell=True)
    p.likelihood_axes.set_xlim(1.90, 2.2)
    p.likelihood_axes.xaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))
    for tick in p.likelihood_axes.get_xticklabels():
        tick.set_rotation(45)
    plt.show()




.. image-sg:: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_002.png
   :alt: Likelihood
   :srcset: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 89-100

No matter which probability density function we choose, for real applications we will never find the exact data 
generated process---neither we will be able to say if we have found it for that matter---due to an oversimplification 
of reality. For most applications, the usual families of probability density functions and transformations of those 
are more than enough approximations for the purpose of the model. In Chapter [sec:model_selection], we will delve into 
this topic.

Once the model is defined we need to infer the set of parameters \( \varTheta \) of the family of density functions 
over the observational space, \( \pi_S(y;\varTheta) \). In the case of the normal family, we need to infer the value 
of the mean, \( \mu \) and standard deviation \( \sigma \). Up to this point, all the description of the probabilistic 
modelling is agnostic in relation to Frequentist or Bayesian views. These two methodologies diverge on how they 
infer \( \varTheta \).

.. GENERATED FROM PYTHON SOURCE LINES 103-112

.. code-block:: python3

    p = PlotPosterior(az_data)
    p.create_figure(figsize=(9, 3), joyplot=False, marginal=False)
    p.plot_normal_likelihood('$\mu$', '$\sigma$', '$y$', iteration=-1, hide_lines=True)
    p.likelihood_axes.set_xlim(1.70, 2.40)
    p.likelihood_axes.xaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))
    for tick in p.likelihood_axes.get_xticklabels():
        tick.set_rotation(45)
    plt.show()




.. image-sg:: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_003.png
   :alt: Likelihood
   :srcset: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 113-119

.. code-block:: python3

    p = PlotPosterior(az_data)

    p.create_figure(figsize=(9, 9), joyplot=True, marginal=False, likelihood=False, n_samples=31)
    p.plot_joy(('$\mu$', '$\sigma$'), '$y$', iteration=14)
    plt.show()




.. image-sg:: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_004.png
   :alt: 1 Intro to Bayesian Inference
   :srcset: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 120-132

.. code-block:: python3

    p = PlotPosterior(az_data)

    p.create_figure(figsize=(9, 5), joyplot=False, marginal=True, likelihood=True)
    p.plot_marginal(var_names=['$\mu$', '$\sigma$'],
                    plot_trace=False, credible_interval=.93, kind='kde',
                    joint_kwargs={'contour': True, 'pcolormesh_kwargs': {}},
                    joint_kwargs_prior={'contour': False, 'pcolormesh_kwargs': {}})

    p.plot_normal_likelihood('$\mu$', '$\sigma$', '$y$', iteration=-1, hide_lines=True)
    p.likelihood_axes.set_xlim(1.70, 2.40)
    plt.show()




.. image-sg:: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_005.png
   :alt: Likelihood
   :srcset: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 133-135

.. code-block:: python3

    az_data = infer_model("u_o", y_obs)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Warmup:   0%|          | 0/200 [00:00, ?it/s]    Warmup:  10%|#         | 20/200 [00:00, 198.08it/s, step size=1.48e-01, acc. prob=0.741]    Warmup:  20%|##        | 40/200 [00:00, 184.15it/s, step size=1.06e-01, acc. prob=0.762]    Warmup:  32%|###2      | 65/200 [00:00, 210.53it/s, step size=1.55e-01, acc. prob=0.775]    Warmup:  44%|####3     | 87/200 [00:00, 205.60it/s, step size=1.50e-01, acc. prob=0.779]    Sample:  54%|#####4    | 108/200 [00:00, 203.81it/s, step size=3.54e-01, acc. prob=0.881]    Sample:  65%|######5   | 130/200 [00:00, 207.85it/s, step size=3.54e-01, acc. prob=0.955]    Sample:  76%|#######6  | 152/200 [00:00, 209.50it/s, step size=3.54e-01, acc. prob=0.948]    Sample:  88%|########7 | 175/200 [00:00, 213.96it/s, step size=3.54e-01, acc. prob=0.929]    Sample:  98%|#########8| 197/200 [00:00, 205.32it/s, step size=3.54e-01, acc. prob=0.941]    Sample: 100%|##########| 200/200 [00:00, 206.86it/s, step size=3.54e-01, acc. prob=0.943]

    /home/leguark/.virtualenvs/gempy-geotop-pilot/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: gempy.-version- is an invalid version and will not be supported in a future release
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 136-149

.. code-block:: python3

    p = PlotPosterior(az_data)

    p.create_figure(figsize=(9, 5), joyplot=False, marginal=True, likelihood=True)
    p.plot_marginal(var_names=['$\mu$', '$\sigma$'],
                    plot_trace=False, credible_interval=.93, kind='kde',
                    joint_kwargs={'contour': True, 'pcolormesh_kwargs': {}},
                    joint_kwargs_prior={'contour': False, 'pcolormesh_kwargs': {}})

    p.axjoin.set_xlim(1.96, 2.22)
    p.plot_normal_likelihood('$\mu$', '$\sigma$', '$y$', iteration=-6, hide_lines=True)
    p.likelihood_axes.set_xlim(1.70, 2.40)
    plt.show()




.. image-sg:: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_006.png
   :alt: Likelihood
   :srcset: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_006.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 150-164

.. code-block:: python3

    az_data = infer_model("u_s", y_obs_list)
    p = PlotPosterior(az_data)

    p.create_figure(figsize=(9, 5), joyplot=False, marginal=True, likelihood=True)
    p.plot_marginal(var_names=['$\mu$', '$\sigma$'],
                    plot_trace=False, credible_interval=.93, kind='kde',
                    joint_kwargs={'contour': True, 'pcolormesh_kwargs': {}},
                    joint_kwargs_prior={'contour': False, 'pcolormesh_kwargs': {}})

    p.plot_normal_likelihood('$\mu$', '$\sigma$', '$y$', iteration=-5, hide_lines=True)
    p.axjoin.set_xlim(1.96, 2.22)
    p.likelihood_axes.set_xlim(1.70, 2.4)
    plt.show()




.. image-sg:: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_007.png
   :alt: Likelihood
   :srcset: /examples_intro/1-bayesian_basics/images/sphx_glr_1_Intro_to_Bayesian_Inference_007.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Warmup:   0%|          | 0/200 [00:00, ?it/s]    Warmup:  15%|#5        | 30/200 [00:00, 296.77it/s, step size=3.21e-02, acc. prob=0.764]    Warmup:  30%|###       | 60/200 [00:00, 176.52it/s, step size=4.77e-03, acc. prob=0.763]    Warmup:  40%|####      | 81/200 [00:00, 160.87it/s, step size=8.71e-03, acc. prob=0.773]    Sample:  54%|#####3    | 107/200 [00:00, 187.75it/s, step size=2.42e-01, acc. prob=0.971]    Sample:  70%|#######   | 140/200 [00:00, 228.29it/s, step size=2.42e-01, acc. prob=0.958]    Sample:  90%|######### | 181/200 [00:00, 280.82it/s, step size=2.42e-01, acc. prob=0.951]    Sample: 100%|##########| 200/200 [00:00, 241.36it/s, step size=2.42e-01, acc. prob=0.954]

    /home/leguark/.virtualenvs/gempy-geotop-pilot/lib/python3.10/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 165-175

License
=======
The code in this case study is copyrighted by Miguel de la Varga and licensed under the new BSD (3-clause) license:

https://opensource.org/licenses/BSD-3-Clause

The text and figures in this case study are copyrighted by Miguel de la Varga and licensed under the CC BY-NC 4.0 license:

https://creativecommons.org/licenses/by-nc/4.0/
Make sure to replace the links with actual hyperlinks if you're using a platform that supports it (e.g., Markdown or HTML). Otherwise, the plain URLs work fine for plain text.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  13.375 seconds)


.. _sphx_glr_download_examples_intro_1-bayesian_basics_1_Intro_to_Bayesian_Inference.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 1_Intro_to_Bayesian_Inference.py <1_Intro_to_Bayesian_Inference.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 1_Intro_to_Bayesian_Inference.ipynb <1_Intro_to_Bayesian_Inference.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
